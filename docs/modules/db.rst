API - Database (alpha)
=========================

This is the alpha version of database management system.
If you have trouble, you can ask for help on `fangde.liu@outlook.com <fangde.liu@outlook.com>`_ .

.. note::
   We are writing up the documentation, please wait in patient.


Why TensorDB
----------------

TensorLayer is designed for real world production, capable of large scale machine learning applications. 
TensorDB is introduced to address many data management challenges in the large scale machine learning projects, such as: 

1. How to find the training data from the enterprise data warehouse. 
2. How to load the datasets  so large that are  beyond the storage limitation of one computer.
3. How can we manage different models with version control, and compare them.
4. How to automate the whole training, evaluaiton and deployment of machine learning models.

In TensorLayer system, we introduce the database technology to the challenges above.

TensorDB is designed by following three principles.

Everything is Data
^^^^^^^^^^^^^^^^^^

TensorDB is a data warehouse that stores and captures the whole machine learning development process. The data inside tensordb can be catagloried as:

1. Data and Labels: This includes all the data for training, validation and prediction. The labels can be manually specified or generated by model prediction.
2. Model Architecture: TensorDB inlcudes a repo that stores the different model architecture, enable user to reuse many machine learning models.
3. Model Parameters: This tables stores all the model parameters of each epoch in the training step.
4. Jobs: All the computation tasks  are divided  into several small jobs. Each jobs constains the necessary information such as hyper-parameter for  training or validation. For a training job, typical information includes training data , the model parameter, the model architecture, how many epochs the training want to do.  Validation jobs and inference jobs are specified in a similar way.
5. Logs: The logs store all the  metric of each machine learning model, such as the time stamp, step time and accuracy of each batch or epoch.

TensorDB in principal is a keyword based search engine. Each model, parameter, or training data are assigned with many tags.
The storage system organize data in two layers. The top layer is the index layer, which stores all the tags and reference to the blob storage. The index layer is implemented based on NoSQL document database such as Mongodb. The second layer is blob storage, which stores  videos, medical images or label masks in large chunk size, which is usually implemented as file system. The open source implementation of TensorDB is based on MongoDB. The blob data system is based on the gridfs while the tags are stored as documents.


Everying is identified by Query
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Within TensorDB framework, any entity within the data warehouse, such as the data, model or jobs are specified by the database query language.  
As a reference,  the query is more space efficient for storage  and it can specify multiple objects in a concise way.
Another advantage of such a design is to enable a highly flexible software system.
data, model architecture and training are interchangeable.
Many work can be implemented by simply rewire different components, and many new applications can be implemented just by update the query without modification of any applicaition code.

An pulling based Stream processing pipeline.
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For large dataset training, TendsorDB provides a stream interface, which in theory can be used to support unlimited large dataset.
TensorDB provides a streaming interface, implemented as python generators, which keeps return the new data during training.
With stream interface, the idea of epoch does not apply anymore, instead, we specify the batchize and treat a large number of steps as an epoch.

Many techniques are introduced behind the streaming interface to optimise performance.
The stream is implemented based on the database cursor technology.
For every data query, only the cursors are returned immediately, not the actual query results. 
The acutal data are  loaded when the generators are evaluated. 

The data loading is further optimised in many ways:

1. Data are compressed and decompressed, 
2. The data are loaded in bulk model to further optimise the IO traffic 
3. The data argumentation or random sampling are computed on the fly, only after the data are loaded in the local computer memory. 
4. We also introduced simple cache system that stores the recent blob data.


Based on the stream interface, continous machine learning system can be easily implemented. 
On a distributed system, the model training, validation and deployment can be run by different computers which are all running continuously.
The trainer can keeps on optimising the models with new added data, the evaluation keeps on evaluating the recent generated models and the deployment system keeps pulling the best models from the TensorDB warehouse.


Preparation
--------------

In principle, TensorDB is can be implemented based on any document oriented NoSQL database system.
The exisitng implementation is based on Mongodb.
Further implementaiton on other database will be released depends on progress.
It will be stragihtford to port the tensorDB system to google cloud , aws and azure.

The following tutorials are based on the MongoDb implmenetation.


Install MongoDB
^^^^^^^^^^^^^^^^^

The installation instruction of Mongodb can be found at
`MongoDB Docs <https://docs.mongodb.com/manual/installation/>`_
there are also managed mongodb service from amazon or gcp, such as the mongo atlas from mongodb

User can also user docker, which is a powerful tool for `deploy software <https://hub.docker.com/_/mongo/>`_ .

After install mongodb, a mongod db management tool with graphic user interface will be extremely valuale.

Users can install the Studio3T( mongochef), which is powerful user interface tool for mongodb and it is free for none commerical usage
`studio3t <https://studio3t.com/>`_


Start MongoDB service
^^^^^^^^^^^^^^^^^^^^^^^^

After mongodb is installed, you shoud start the database.

``mongod start``

You can specificy the path the database files with ``-d``  flag

Quick Start
-----------

A fully working example with mnist training set is the _TensorLabDemo.ipnb_


Connect to database
^^^^^^^^^^^^^^^^^^^^^^^

To use TensorDB mongodb implmentaiton,  you need pymongo client.

you can install it by 

.. code-block:: bash

  pip install pymongo
  pip install lz4


it is very strateford to connected to the TensorDB system.
you can try the following code

.. code-block:: python

  from tensorlayer.db import TensorDB
  db = TensorDB(ip='127.0.0.1', port=27017, db_name='your_db', user_name=None, password=None, studyID='ministMLP')
  

The ``ip`` is the ip address of the database, and ``port`` number is number of mongodb.
You may need to specificy the database name and studyid.
The study id is an unique identifier for an experiement.

TensorDB stores different study in one data warehouse. 
This has pros and cons, the benefits are that it is more convient for us to compare different studies.
Suppose the each study we try a different model architecutre, we evaluate different model architecture by visiting just one database.


logs and parameters 
^^^^^^^^^^^^^^^^^^^^^^^

The basic application is use TensorDB to save the model parameters and training/evaluation/testing logs.
to use tensorDB, this can be easily done by replacing the print function by the db.log function

For save the trainning log, we have
``db.train_log``

and 

``db. save_parameter``

methods

Suppose we save the log each step and save the parameters each epoch, we can have the code like this

.. code-block:: python

   for epoch in range(0,epoch_count):
      [~,ac]=sess.run([train_op,loss],feed_dict({x:x,y:y_}
      db.train_log({'accuracy':ac})
   db.save_parameter(sess.run(network.all_parameters),{'acc':ac})
   
the code for save validation log and test log are similar.


Model Architecture and Jobs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TensorDb also support managment of  the model architecture and jobs system
in the current version, both the model architecture and job are just simply strings.
it is up to the user to specifiy how to convert the string back to models or job.
for example, in many our our cases, we just simpliy specify the python code.

.. code-block:: python

   code= '''
   print "hello
   '''
   db.save_model_architecutre(code,{'name':'print'}
   
   c,fid = db.find_model_architecutre({'name':'print'})
   exec c
   
   db.push_job(code,{'type':'train'})
   
   ## worker
   code = db.pop_job()
   exec code
   
   
Database Interface
^^^^^^^^^^^^^^^^^^^^^^^

The trainning set is managed by a seperate database.
each application has its own database.
However, all the database should support two interface,
1. find_data,
2. data_generator

and example for minist dataset is include in the TensorLabDemo code



Data Importing
^^^^^^^^^^^^^^^
With a database, the development workflow is very flexible. 
As long as the content in the database in the same, user can use whatever tools to write into the database

the TesorLabDemo has an import data interface, which allow the user to injecting data in future

user can import data by the following code

.. code-block:: python

  db.import_data(X,y,{'type':'train'})



Application Framework
----------------------

In fact, in real application, we rarely code everything from scrach and using the tensorDB interface directly.
as demostrate in the TensorLabDemo

we implemented 4 class each with a well defined interace.
1. The dataset. 
2. The TensorDb
3. The Model, a model is loggically a full class object that can be trained, evaluate and deployed. It has property like parameters
4. The DBLogger, which is connector from model to tensorDB, which is implemented as callback functions, automatically called at each batch_step and each epoch.

users can based on the TensorLabDemo code, overrite the interface to suits their own applicaions needs.

when training, the overall archtiecture is 
first, find a data generator from the dataset module

.. code-block:: python

  g=datase.data_generator({"type":[your type]})

then intialize a model with a name

.. code-block:: python

  m=model('mytest')

during training, connected the db logger and tensordb togehter

.. code-block:: python

  m.fit_generator(g,dblogger(tensordb,m),1000,100)

if the work is distributed, we have to save the model archtiecture and reload and excute it

.. code-block:: python

   db.save_model_architecture(code,{'name':'mlp'})
   db.push_job({'name':'mlp'},{'type':XXXX},{'batch:1000','epoch':100)


the worker will run the job as the following code

.. code-block:: python

   j=job.pop
   g=dataset.data_generator(j.filter)
   c=tensordb.load_model_architecutre(j.march)
   exec c
   m=model()
   m.fit_generator(g,dblooger(tensordb,m),j.bach_size,j.epoch}





.. automodule:: tensorlayer.db

.. autoclass:: TensorDB
   :members:
